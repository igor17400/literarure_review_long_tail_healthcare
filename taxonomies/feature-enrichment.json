{
  "category": "Feature Enrichment",
  "description": "Representation learning to produce more discriminative embeddings for minority classes",
  "papers": [
    {
      "id": "pan2025long",
      "title": "Long-tailed medical diagnosis with relation-aware representation learning and iterative classifier calibration",
      "authors": "Pan, Li and Zhang, Yupei and Yang, Qiushi and Li, Tan and Chen, Zhen",
      "year": "2025",
      "venue": "Computers in Biology and Medicine",
      "abstract": "Recently computer-aided diagnosis has demonstrated promising performance, effectively alleviating the workload of clinicians. However, the inherent sample imbalance among different diseases leads algorithms biased to the majority categories, leading to poor performance for rare categories. Existing works formulated this challenge as a long-tailed problem and attempted to tackle it by decoupling the feature representation and classification. Yet, due to the imbalanced distribution and limited samples from tail classes, these works are prone to biased representation learning and insufficient classifier calibration. To tackle these problems, we propose a new Long-tailed Medical Diagnosis (LMD) framework for balanced medical image classification on long-tailed datasets. In the initial stage, we develop a Relation-aware Representation Learning (RRL) scheme to boost the representation ability by encouraging the encoder to capture intrinsic semantic features through different data augmentations. In the subsequent stage, we propose an Iterative Classifier Calibration (ICC) scheme to calibrate the classifier iteratively. This is achieved by generating a large number of balanced virtual features and fine-tuning the encoder using an Expectation-Maximization manner. The proposed ICC compensates for minority categories to facilitate unbiased classifier optimization while maintaining the diagnostic knowledge in majority classes. Comprehensive experiments on three public long-tailed medical datasets demonstrate that our LMD framework significantly surpasses state-of-the-art approaches. The source code can be accessed at https://github.com/peterlipan/LMD.",
      "bibtex": "@article{pan2025long,\n  title={Long-tailed medical diagnosis with relation-aware representation learning and iterative classifier calibration},\n  author={Pan, Li and Zhang, Yupei and Yang, Qiushi and Li, Tan and Chen, Zhen},\n  journal={Computers in Biology and Medicine},\n  volume={188},\n  pages={109772},\n  year={2025},\n  publisher={Elsevier}\n  abstract={Recently computer-aided diagnosis has demonstrated promising performance, effectively alleviating the workload of clinicians. However, the inherent sample imbalance among different diseases leads algorithms biased to the majority categories, leading to poor performance for rare categories. Existing works formulated this challenge as a long-tailed problem and attempted to tackle it by decoupling the feature representation and classification. Yet, due to the imbalanced distribution and limited samples from tail classes, these works are prone to biased representation learning and insufficient classifier calibration. To tackle these problems, we propose a new Long-tailed Medical Diagnosis (LMD) framework for balanced medical image classification on long-tailed datasets. In the initial stage, we develop a Relation-aware Representation Learning (RRL) scheme to boost the representation ability by encouraging the encoder to capture intrinsic semantic features through different data augmentations. In the subsequent stage, we propose an Iterative Classifier Calibration (ICC) scheme to calibrate the classifier iteratively. This is achieved by generating a large number of balanced virtual features and fine-tuning the encoder using an Expectation-Maximization manner. The proposed ICC compensates for minority categories to facilitate unbiased classifier optimization while maintaining the diagnostic knowledge in majority classes. Comprehensive experiments on three public long-tailed medical datasets demonstrate that our LMD framework significantly surpasses state-of-the-art approaches. The source code can be accessed at https://github.com/peterlipan/LMD.}\n}"
    },
    {
      "id": "zheng2024large",
      "title": "Large-scale long-tailed disease diagnosis on radiology images",
      "authors": "Zheng, Qiaoyu and Zhao, Weike and Wu, Chaoyi and Zhang, Xiaoman and Dai, Lisong and Guan, Hengyu and Li, Yuehua and Zhang, Ya and Wang, Yanfeng and Xie, Weidi",
      "year": "2024",
      "venue": "Nature Communications",
      "abstract": "Developing a generalist radiology diagnosis system can greatly enhance clinical diagnostics. In this paper, we introduce RadDiag, a foundational model supporting 2D and 3D inputs across various modalities and anatomies, using a transformer-based fusion module for comprehensive disease diagnosis. Due to patient privacy concerns and the lack of large-scale radiology diagnosis datasets, we utilize high-quality, clinician-reviewed radiological images available online with diagnosis labels. Our dataset, RP3D-DiagDS, contains 40,936 cases with 195,010 scans covering 5568 disorders (930 unique ICD-10-CM codes). Experimentally, our RadDiag achieves 95.14% AUC on internal evaluation with the knowledge-enhancement strategy. Additionally, RadDiag can be zero-shot applied or fine-tuned to external diagnosis datasets sourced from various medical centers, demonstrating state-of-the-art results. In conclusion, we show that publicly shared medical data on the Internet is a tremendous and valuable resource that can potentially support building strong models for image understanding in healthcare.",
      "bibtex": "@article{zheng2024large,\n  title={Large-scale long-tailed disease diagnosis on radiology images},\n  author={Zheng, Qiaoyu and Zhao, Weike and Wu, Chaoyi and Zhang, Xiaoman and Dai, Lisong and Guan, Hengyu and Li, Yuehua and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},\n  journal={Nature Communications},\n  volume={15},\n  number={1},\n  pages={10147},\n  year={2024},\n  publisher={Nature Publishing Group UK London}\n  abstract={Developing a generalist radiology diagnosis system can greatly enhance clinical diagnostics. In this paper, we introduce RadDiag, a foundational model supporting 2D and 3D inputs across various modalities and anatomies, using a transformer-based fusion module for comprehensive disease diagnosis. Due to patient privacy concerns and the lack of large-scale radiology diagnosis datasets, we utilize high-quality, clinician-reviewed radiological images available online with diagnosis labels. Our dataset, RP3D-DiagDS, contains 40,936 cases with 195,010 scans covering 5568 disorders (930 unique ICD-10-CM codes). Experimentally, our RadDiag achieves 95.14% AUC on internal evaluation with the knowledge-enhancement strategy. Additionally, RadDiag can be zero-shot applied or fine-tuned to external diagnosis datasets sourced from various medical centers, demonstrating state-of-the-art results. In conclusion, we show that publicly shared medical data on the Internet is a tremendous and valuable resource that can potentially support building strong models for image understanding in healthcare.}\n}"
    },
    {
      "id": "ayzenberg2025protosam",
      "title": "ProtoSAM for automated one shot medical image segmentation using foundational models",
      "authors": "Ayzenberg, Lev and Giryes, Raja and Greenspan, Hayit",
      "year": "2025",
      "venue": "Scientific Reports",
      "abstract": "This work presents an advance in one-shot medical image segmentation, where a single image-label sample from a new site is used for finetuning the solution - particularly valuable in scenarios where labeled data is scarce or rapid adaptation to new classes and sites is required. We introduce ProtoSAM, a novel, fully automated framework, for one-shot medical image segmentation that combines Prototypical networks, known for few-shot segmentation, with the Segment Anything Model (SAM), a natural image foundation model for segmentation. The proposed method creates an initial coarse segmentation mask using the ALPnet prototypical network, augmented with a DINOv2 encoder. Following the extraction of an initial mask, prompts are extracted, such as points and bounding boxes, which are then input into SAM. We present extensive validation on multiple datasets including CT, MRI, and endoscopy images, demonstrating state-of-the-art results in many scenarios. Our results show that an untrained ProtoSAM can match or exceed the performance of existing one-shot trained methods, with further improvements possible through self-supervised finetuning of the encoder. Our code is available at: https://github.com/levayz/ProtoSAM/.",
      "bibtex": "@article{ayzenberg2025protosam,\n  title={ProtoSAM for automated one shot medical image segmentation using foundational models},\n  author={Ayzenberg, Lev and Giryes, Raja and Greenspan, Hayit},\n  journal={Scientific Reports},\n  volume={15},\n  number={1},\n  pages={41482},\n  year={2025},\n  publisher={Nature Publishing Group UK London}\n  abstract={This work presents an advance in one-shot medical image segmentation, where a single image-label sample from a new site is used for finetuning the solution - particularly valuable in scenarios where labeled data is scarce or rapid adaptation to new classes and sites is required. We introduce ProtoSAM, a novel, fully automated framework, for one-shot medical image segmentation that combines Prototypical networks, known for few-shot segmentation, with the Segment Anything Model (SAM), a natural image foundation model for segmentation. The proposed method creates an initial coarse segmentation mask using the ALPnet prototypical network, augmented with a DINOv2 encoder. Following the extraction of an initial mask, prompts are extracted, such as points and bounding boxes, which are then input into SAM. We present extensive validation on multiple datasets including CT, MRI, and endoscopy images, demonstrating state-of-the-art results in many scenarios. Our results show that an untrained ProtoSAM can match or exceed the performance of existing one-shot trained methods, with further improvements possible through self-supervised finetuning of the encoder. Our code is available at: https://github.com/levayz/ProtoSAM/.}\n}"
    },
    {
      "id": "jang2024significantly",
      "title": "Significantly improving zero-shot X-ray pathology classification via fine-tuning pre-trained image-text encoders",
      "authors": "Jang, Jongseong and Kyung, Daeun and Kim, Seung Hwan and Lee, Honglak and Bae, Kyunghoon and Choi, Edward",
      "year": "2024",
      "venue": "Scientific Reports",
      "abstract": "Deep neural networks are increasingly used in medical imaging for tasks such as pathological classification, but they face challenges due to the scarcity of high-quality, expert-labeled training data. Recent efforts have utilized pre-trained contrastive image-text models like CLIP, adapting them for medical use by fine-tuning the model with chest X-ray images and corresponding reports for zero-shot pathology classification, thus eliminating the need for pathology-specific annotations. However, most studies continue to use the same contrastive learning objectives as in the general domain, overlooking the multi-labeled nature of medical image-report pairs. In this paper, we propose a new fine-tuning strategy that includes positive-pair loss relaxation and random sentence sampling. We aim to improve the performance of zero-shot pathology classification without relying on external knowledge. Our method can be applied to any pre-trained contrastive image-text encoder and easily transferred to out-of-domain datasets without further training, as it does not use external data. Our approach consistently improves overall zero-shot pathology classification across four chest X-ray datasets and three pre-trained models, with an average macro AUROC increase of 4.3%. Additionally, our method outperforms the state-of-the-art and marginally surpasses board-certified radiologists in zero-shot classification for the five competition pathologies in the CheXpert dataset.",
      "bibtex": "@article{jang2024significantly,\n  title={Significantly improving zero-shot X-ray pathology classification via fine-tuning pre-trained image-text encoders},\n  author={Jang, Jongseong and Kyung, Daeun and Kim, Seung Hwan and Lee, Honglak and Bae, Kyunghoon and Choi, Edward},\n  journal={Scientific Reports},\n  volume={14},\n  number={1},\n  pages={23199},\n  year={2024},\n  publisher={Nature Publishing Group UK London}\n  abstract={Deep neural networks are increasingly used in medical imaging for tasks such as pathological classification, but they face challenges due to the scarcity of high-quality, expert-labeled training data. Recent efforts have utilized pre-trained contrastive image-text models like CLIP, adapting them for medical use by fine-tuning the model with chest X-ray images and corresponding reports for zero-shot pathology classification, thus eliminating the need for pathology-specific annotations. However, most studies continue to use the same contrastive learning objectives as in the general domain, overlooking the multi-labeled nature of medical image-report pairs. In this paper, we propose a new fine-tuning strategy that includes positive-pair loss relaxation and random sentence sampling. We aim to improve the performance of zero-shot pathology classification without relying on external knowledge. Our method can be applied to any pre-trained contrastive image-text encoder and easily transferred to out-of-domain datasets without further training, as it does not use external data. Our approach consistently improves overall zero-shot pathology classification across four chest X-ray datasets and three pre-trained models, with an average macro AUROC increase of 4.3%. Additionally, our method outperforms the state-of-the-art and marginally surpasses board-certified radiologists in zero-shot classification for the five competition pathologies in the CheXpert dataset.}\n}"
    }
  ]
}