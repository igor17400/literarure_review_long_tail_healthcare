{
  "category": "Fairness, Bias, and Health Equity",
  "description": "Methods addressing demographic imbalances and algorithmic fairness in healthcare",
  "papers": [
    {
      "id": "seyyed2020chexclusion",
      "title": "CheXclusion: Fairness gaps in deep chest X-ray classifiers",
      "authors": "Seyyed-Kalantari, Laleh and Liu, Guanxiong and McDermott, Matthew and Chen, Irene Y and Ghassemi, Marzyeh",
      "year": "2020",
      "venue": "BIOCOMPUTING 2021: proceedings of the Pacific symposium",
      "abstract": "Machine learning systems have received much attention recently for their ability to achieve expert-level performance on clinical tasks, particularly in medical imaging. Here, we examine the extent to which state-of-the-art deep learning classifiers trained to yield diagnostic labels from X-ray images are biased with respect to protected attributes. We train convolution neural networks to predict 14 diagnostic labels in 3 prominent public chest X-ray datasets: MIMIC-CXR, Chest-Xray8, CheXpert, as well as a multi-site aggregation of all those datasets. We evaluate the TPR disparity – the difference in true positive rates (TPR) – among different protected attributes such as patient sex, age, race, and insurance type as a proxy for socioeconomic status. We demonstrate that TPR disparities exist in the stateof-the-art classifiers in all datasets, for all clinical tasks, and all subgroups. A multi-source dataset corresponds to the smallest disparities, suggesting one way to reduce bias. We find that TPR disparities are not significantly correlated with a subgroup’s proportional disease burden. As clinical models move from papers to products, we encourage clinical decision makers to carefully audit for algorithmic disparities prior to deployment. Our code can be found at, https://github.com/LalehSeyyed/CheXclusion.",
      "bibtex": "@inproceedings{seyyed2020chexclusion,\n  title={CheXclusion: Fairness gaps in deep chest X-ray classifiers},\n  author={Seyyed-Kalantari, Laleh and Liu, Guanxiong and McDermott, Matthew and Chen, Irene Y and Ghassemi, Marzyeh},\n  booktitle={BIOCOMPUTING 2021: proceedings of the Pacific symposium},\n  pages={232--243},\n  year={2020},\n  organization={World Scientific}\n  abstract={Machine learning systems have received much attention recently for their ability to achieve expert-level performance on clinical tasks, particularly in medical imaging. Here, we examine the extent to which state-of-the-art deep learning classifiers trained to yield diagnostic labels from X-ray images are biased with respect to protected attributes. We train convolution neural networks to predict 14 diagnostic labels in 3 prominent public chest X-ray datasets: MIMIC-CXR, Chest-Xray8, CheXpert, as well as a multi-site aggregation of all those datasets. We evaluate the TPR disparity – the difference in true positive rates (TPR) – among different protected attributes such as patient sex, age, race, and insurance type as a proxy for socioeconomic status. We demonstrate that TPR disparities exist in the stateof-the-art classifiers in all datasets, for all clinical tasks, and all subgroups. A multi-source dataset corresponds to the smallest disparities, suggesting one way to reduce bias. We find that TPR disparities are not significantly correlated with a subgroup’s proportional disease burden. As clinical models move from papers to products, we encourage clinical decision makers to carefully audit for algorithmic disparities prior to deployment. Our code can be found at, https://github.com/LalehSeyyed/CheXclusion.}\n}"
    },
    {
      "id": "guevara2024large",
      "title": "Large language models to identify social determinants of health in electronic health records",
      "authors": "Guevara, Marco and Chen, Shan and Thomas, Spencer and Chaunzwa, Tafadzwa L and Franco, Idalid and Kann, Benjamin H and Moningi, Shalini and Qian, Jack M and Goldstein, Madeleine and Harper, Susan and others",
      "year": "2024",
      "venue": "NPJ digital medicine",
      "abstract": "Abstract not available.",
      "bibtex": "@article{guevara2024large,\n  title={Large language models to identify social determinants of health in electronic health records},\n  author={Guevara, Marco and Chen, Shan and Thomas, Spencer and Chaunzwa, Tafadzwa L and Franco, Idalid and Kann, Benjamin H and Moningi, Shalini and Qian, Jack M and Goldstein, Madeleine and Harper, Susan and others},\n  journal={NPJ digital medicine},\n  volume={7},\n  number={1},\n  pages={6},\n  year={2024},\n  publisher={Nature Publishing Group UK London}\n  abstract={}Social determinants of health (SDoH) play a critical role in patient outcomes, yet their documentation is often missing or incomplete in the structured data of electronic health records (EHRs). Large language models (LLMs) could enable high-throughput extraction of SDoH from the EHR to support research and clinical care. However, class imbalance and data limitations present challenges for this sparsely documented yet critical information. Here, we investigated the optimal methods for using LLMs to extract six SDoH categories from narrative text in the EHR: employment, housing, transportation, parental status, relationship, and social support. The best-performing models were fine-tuned Flan-T5 XL for any SDoH mentions (macro-F1 0.71), and Flan-T5 XXL for adverse SDoH mentions (macro-F1 0.70). Adding LLM-generated synthetic data to training varied across models and architecture, but improved the performance of smaller Flan-T5 models (delta F1 + 0.12 to +0.23). Our best-fine-tuned models outperformed zero- and few-shot performance of ChatGPT-family models in the zero- and few-shot setting, except GPT4 with 10-shot prompting for adverse SDoH. Fine-tuned models were less likely than ChatGPT to change their prediction when race/ethnicity and gender descriptors were added to the text, suggesting less algorithmic bias (p < 0.05). Our models identified 93.8% of patients with adverse SDoH, while ICD-10 codes captured 2.0%. These results demonstrate the potential of LLMs in improving real-world evidence on SDoH and assisting in identifying patients who could benefit from resource support.\n}"
    },
    {
      "id": "parikh2019addressing",
      "title": "Addressing bias in artificial intelligence in health care",
      "authors": "Parikh, Ravi B and Teeple, Stephanie and Navathe, Amol S",
      "year": "2019",
      "venue": "Jama",
      "abstract": "Recent scrutiny of artificial intelligence (AI)–based facial recognition software has renewed concerns about the unintended effects of AI on social bias and inequity. Academic and government officials have raised concerns over racial and gender bias in several AI-based technologies, including internet search engines and algorithms to predict risk of criminal behavior. Companies like IBM and Microsoft have made public commitments to “de-bias” their technologies, whereas Amazon mounted a public campaign criticizing such research. As AI applications gain traction in medicine, clinicians and health system leaders have raised similar concerns over automating and propagating existing biases.1 But is AI the problem? Or can it be part of the solution? While potentially inadvertently contributing to bias, AI technologies, when used responsibly, may also help counteract the risk of bias in unique ways. Using AI to identify bias in health care may help identify interventions that could help correct biased clinician decision-making and possibly reduce health disparities.",
      "bibtex": "@article{parikh2019addressing,\n  title={Addressing bias in artificial intelligence in health care},\n  author={Parikh, Ravi B and Teeple, Stephanie and Navathe, Amol S},\n  journal={Jama},\n  volume={322},\n  number={24},\n  pages={2377--2378},\n  year={2019}\n  abstract={Recent scrutiny of artificial intelligence (AI)–based facial recognition software has renewed concerns about the unintended effects of AI on social bias and inequity. Academic and government officials have raised concerns over racial and gender bias in several AI-based technologies, including internet search engines and algorithms to predict risk of criminal behavior. Companies like IBM and Microsoft have made public commitments to “de-bias” their technologies, whereas Amazon mounted a public campaign criticizing such research. As AI applications gain traction in medicine, clinicians and health system leaders have raised similar concerns over automating and propagating existing biases.1 But is AI the problem? Or can it be part of the solution? While potentially inadvertently contributing to bias, AI technologies, when used responsibly, may also help counteract the risk of bias in unique ways. Using AI to identify bias in health care may help identify interventions that could help correct biased clinician decision-making and possibly reduce health disparities.}\n}"
    },
    {
      "id": "puyol2021fairness",
      "title": "Fairness in cardiac MR image analysis: an investigation of bias due to data imbalance in deep learning based segmentation",
      "authors": "Puyol-Ant{\\'o",
      "year": "2021",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "abstract": "Abstract not available.",
      "bibtex": "@inproceedings{puyol2021fairness,\n  title={Fairness in cardiac MR image analysis: an investigation of bias due to data imbalance in deep learning based segmentation},\n  author={Puyol-Ant{\\'o}n, Esther and Ruijsink, Bram and Piechnik, Stefan K and Neubauer, Stefan and Petersen, Steffen E and Razavi, Reza and King, Andrew P},\n  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},\n  pages={413--423},\n  year={2021},\n  organization={Springer}\n  absract={The subject of ‘fairness’ in artificial intelligence (AI) refers to assessing AI algorithms for potential bias based on demographic characteristics such as race and gender, and the development of algorithms to address this bias. Most applications to date have been in computer vision, although some work in healthcare has started to emerge. The use of deep learning (DL) in cardiac MR segmentation has led to impressive results in recent years, and such techniques are starting to be translated into clinical practice. However, no work has yet investigated the fairness of such models. In this work, we perform such an analysis for racial/gender groups, focusing on the problem of training data imbalance, using a nnU-Net model trained and evaluated on cine short axis cardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from 6 different racial groups. We find statistically significant differences in Dice performance between different racial groups. To reduce the racial bias, we investigated three strategies: (1) stratified batch sampling, in which batch sampling is stratified to ensure balance between racial groups; (2) fair meta-learning for segmentation, in which a DL classifier is trained to classify race and jointly optimized with the segmentation model; and (3) protected group models, in which a different segmentation model is trained for each racial group. We also compared the results to the scenario where we have a perfectly balanced database. To assess fairness we used the standard deviation (SD) and skewed error ratio (SER) of the average Dice values. Our results demonstrate that the racial bias results from the use of imbalanced training data, and that all proposed bias mitigation strategies improved fairness, with the best SD and SER resulting from the use of protected group models.}\n}"
    },
    {
      "id": "rajkomar2018ensuring",
      "title": "Ensuring fairness in machine learning to advance health equity",
      "authors": "Rajkomar, Alvin and Hardt, Michaela and Howell, Michael D and Corrado, Greg and Chin, Marshall H",
      "year": "2018",
      "venue": "Annals of internal medicine",
      "abstract": "Machine learning is used increasingly in clinical care to improve diagnosis, treatment selection, and health system efficiency. Because machine-learning models learn from historically collected data, populations that have experienced human and structural biases in the past-called protected groups-are vulnerable to harm by incorrect predictions or withholding of resources. This article describes how model design, biases in data, and the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Rather than simply guarding against these harms passively, machine-learning systems should be used proactively to advance health equity. For that goal to be achieved, principles of distributive justice must be incorporated into model design, deployment, and evaluation. The article describes several technical implementations of distributive justice-specifically those that ensure equality in patient outcomes, performance, and resource allocation-and guides clinicians as to when they should prioritize each principle. Machine learning is providing increasingly sophisticated decision support and population-level monitoring, and it should encode principles of justice to ensure that models benefit all patients.",
      "bibtex": "@article{rajkomar2018ensuring,\n  title={Ensuring fairness in machine learning to advance health equity},\n  author={Rajkomar, Alvin and Hardt, Michaela and Howell, Michael D and Corrado, Greg and Chin, Marshall H},\n  journal={Annals of internal medicine},\n  volume={169},\n  number={12},\n  pages={866--872},\n  year={2018},\n  publisher={American College of Physicians}\n  abstract={Machine learning is used increasingly in clinical care to improve diagnosis, treatment selection, and health system efficiency. Because machine-learning models learn from historically collected data, populations that have experienced human and structural biases in the past-called protected groups-are vulnerable to harm by incorrect predictions or withholding of resources. This article describes how model design, biases in data, and the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Rather than simply guarding against these harms passively, machine-learning systems should be used proactively to advance health equity. For that goal to be achieved, principles of distributive justice must be incorporated into model design, deployment, and evaluation. The article describes several technical implementations of distributive justice-specifically those that ensure equality in patient outcomes, performance, and resource allocation-and guides clinicians as to when they should prioritize each principle. Machine learning is providing increasingly sophisticated decision support and population-level monitoring, and it should encode principles of justice to ensure that models benefit all patients.}\n}"
    },
    {
      "id": "robitschek2025large",
      "title": "A large language model-based approach to quantifying the effects of social determinants in liver transplant decisions",
      "authors": "Robitschek, Emily and Bastani, Asal and Horwath, Kathryn and Sordean, Savyon and Pletcher, Mark J and Lai, Jennifer C and Galletta, Sergio and Ash, Elliott and Ge, Jin and Chen, Irene Y",
      "year": "2025",
      "venue": "npj Digital Medicine",
      "abstract": "Psychosocial risk factors and social determinants of health (SDOH) contribute to persistent disparities in liver transplantation access. We developed a large language model framework to extract and analyze how these factors influence care trajectories. Prevalence of key modifiable barriers varied by demographics: social support gaps (35.4%, disproportionately affecting females), recent substance use (14.2–22.7%), and mental health challenges (17.6%, with Hispanic/Latino treatment gaps). Each factor was associated with 5–14 percentage point reductions in listing probability, comparable to clinical metrics. Psychosocial risk and SDOH factors explained 42.6% of racial disparities in listing decisions for Asian patients, exceeding liver health metrics (36.8%) and contributing to 94.6% collective explanation of differences. Priority interventions should target caregiver support, substance use, mental health, and patient education. This framework for systematically analyzing patient circumstances could enhance understanding of care decisions and health disparities.",
      "bibtex": "@article{robitschek2025large,\n  title={A large language model-based approach to quantifying the effects of social determinants in liver transplant decisions},\n  author={Robitschek, Emily and Bastani, Asal and Horwath, Kathryn and Sordean, Savyon and Pletcher, Mark J and Lai, Jennifer C and Galletta, Sergio and Ash, Elliott and Ge, Jin and Chen, Irene Y},\n  journal={npj Digital Medicine},\n  volume={8},\n  number={1},\n  pages={665},\n  year={2025},\n  publisher={Nature Publishing Group UK London}\n  abstract={Psychosocial risk factors and social determinants of health (SDOH) contribute to persistent disparities in liver transplantation access. We developed a large language model framework to extract and analyze how these factors influence care trajectories. Prevalence of key modifiable barriers varied by demographics: social support gaps (35.4%, disproportionately affecting females), recent substance use (14.2–22.7%), and mental health challenges (17.6%, with Hispanic/Latino treatment gaps). Each factor was associated with 5–14 percentage point reductions in listing probability, comparable to clinical metrics. Psychosocial risk and SDOH factors explained 42.6% of racial disparities in listing decisions for Asian patients, exceeding liver health metrics (36.8%) and contributing to 94.6% collective explanation of differences. Priority interventions should target caregiver support, substance use, mental health, and patient education. This framework for systematically analyzing patient circumstances could enhance understanding of care decisions and health disparities.}\n}"
    }
  ]
}