{
  "category": "Multi-modality",
  "description": "Multi-modal learning approaches addressing missing modalities and class imbalance",
  "papers": [
    {
      "id": "han2024fusemoe",
      "title": "Fusemoe: Mixture-of-experts transformers for fleximodal fusion",
      "authors": "Han, Xing and Nguyen, Huy and Harris, Carl and Ho, Nhat and Saria, Suchi",
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "abstract": "As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in the real world is validated by a diverse set of challenging prediction tasks.",
      "bibtex": "@article{han2024fusemoe,\n  title={Fusemoe: Mixture-of-experts transformers for fleximodal fusion},\n  author={Han, Xing and Nguyen, Huy and Harris, Carl and Ho, Nhat and Saria, Suchi},\n  journal={Advances in Neural Information Processing Systems},\n  volume={37},\n  pages={67850--67900},\n  year={2024}\n  abstract={As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in the real world is validated by a diverse set of challenging prediction tasks.}\n}"
    },
    {
      "id": "holste2024towards",
      "title": "Towards long-tailed, multi-label disease classification from chest X-ray: Overview of the CXR-LT challenge",
      "authors": "Holste, Gregory and Zhou, Yiliang and Wang, Song and Jaiswal, Ajay and Lin, Mingquan and Zhuge, Sherry and Yang, Yuzhe and Kim, Dongkyun and Nguyen-Mau, Trong-Hieu and Tran, Minh-Triet and others",
      "year": "2024",
      "venue": "Medical Image Analysis",
      "abstract": "Many real-world image recognition problems, such as diagnostic medical imaging exams, are “long-tailed” – there are a few common findings followed by many more relatively rare conditions. In chest radiography, diagnosis is both a long-tailed and multi-label problem, as patients often present with multiple findings simultaneously. While researchers have begun to study the problem of long-tailed learning in medical image recognition, few have studied the interaction of label imbalance and label co-occurrence posed by long-tailed, multi-label disease classification. To engage with the research community on this emerging topic, we conducted an open challenge, CXR-LT, on long-tailed, multi-label thorax disease classification from chest X-rays (CXRs). We publicly release a large-scale benchmark dataset of over 350,000 CXRs, each labeled with at least one of 26 clinical findings following a long-tailed distribution. We synthesize common themes of top-performing solutions, providing practical recommendations for long-tailed, multi-label medical image classification. Finally, we use these insights to propose a path forward involving vision-language foundation models for few- and zero-shot disease classification.",
      "bibtex": "@article{holste2024towards,\n  title={Towards long-tailed, multi-label disease classification from chest X-ray: Overview of the CXR-LT challenge},\n  author={Holste, Gregory and Zhou, Yiliang and Wang, Song and Jaiswal, Ajay and Lin, Mingquan and Zhuge, Sherry and Yang, Yuzhe and Kim, Dongkyun and Nguyen-Mau, Trong-Hieu and Tran, Minh-Triet and others},\n  journal={Medical Image Analysis},\n  volume={97},\n  pages={103224},\n  year={2024},\n  publisher={Elsevier}\n  abstract={Many real-world image recognition problems, such as diagnostic medical imaging exams, are “long-tailed” – there are a few common findings followed by many more relatively rare conditions. In chest radiography, diagnosis is both a long-tailed and multi-label problem, as patients often present with multiple findings simultaneously. While researchers have begun to study the problem of long-tailed learning in medical image recognition, few have studied the interaction of label imbalance and label co-occurrence posed by long-tailed, multi-label disease classification. To engage with the research community on this emerging topic, we conducted an open challenge, CXR-LT, on long-tailed, multi-label thorax disease classification from chest X-rays (CXRs). We publicly release a large-scale benchmark dataset of over 350,000 CXRs, each labeled with at least one of 26 clinical findings following a long-tailed distribution. We synthesize common themes of top-performing solutions, providing practical recommendations for long-tailed, multi-label medical image classification. Finally, we use these insights to propose a path forward involving vision-language foundation models for few- and zero-shot disease classification.}\n}"
    },
    {
      "id": "yun2024flex",
      "title": "Flex-moe: Modeling arbitrary modality combination via the flexible mixture-of-experts",
      "authors": "Yun, Sukwon and Choi, Inyoung and Peng, Jie and Wu, Yangfan and Bao, Jingxuan and Zhang, Qiyiwen and Xin, Jiayi and Long, Qi and Chen, Tianlong",
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "abstract": "Multimodal learning has gained increasing importance across various fields, offering the ability to integrate data from diverse sources such as images, text, and personalized records, which are frequently observed in medical domains. However, in scenarios where some modalities are missing, many existing frameworks struggle to accommodate arbitrary modality combinations, often relying heavily on a single modality or complete data. This oversight of potential modality combinations limits their applicability in real-world situations. To address this challenge, we propose Flex-MoE (Flexible Mixture-of-Experts), a new framework designed to flexibly incorporate arbitrary modality combinations while maintaining robustness to missing data. The core idea of Flex-MoE is to first address missing modalities using a new missing modality bank that integrates observed modality combinations with the corresponding missing ones. This is followed by a uniquely designed Sparse MoE framework. Specifically, Flex-MoE first trains experts using samples with all modalities to inject generalized knowledge through the generalized router (G-Router). The S-Router then specializes in handling fewer modality combinations by assigning the top-1 gate to the expert corresponding to the observed modality combination. We evaluate Flex-MoE on the ADNI dataset, which encompasses four modalities in the Alzheimer’s Disease domain, as well as on the MIMIC-IV dataset. The results demonstrate the effectiveness of Flex-MoE, highlighting its ability to model arbitrary modality combinations in diverse missing modality scenarios. Code is available at: https://github.com/UNITES-Lab/flex-moe.",
      "bibtex": "@article{yun2024flex,\n  title={Flex-moe: Modeling arbitrary modality combination via the flexible mixture-of-experts},\n  author={Yun, Sukwon and Choi, Inyoung and Peng, Jie and Wu, Yangfan and Bao, Jingxuan and Zhang, Qiyiwen and Xin, Jiayi and Long, Qi and Chen, Tianlong},\n  journal={Advances in Neural Information Processing Systems},\n  volume={37},\n  pages={98782--98805},\n  year={2024}\n  abstract={Multimodal learning has gained increasing importance across various fields, offering the ability to integrate data from diverse sources such as images, text, and personalized records, which are frequently observed in medical domains. However, in scenarios where some modalities are missing, many existing frameworks struggle to accommodate arbitrary modality combinations, often relying heavily on a single modality or complete data. This oversight of potential modality combinations limits their applicability in real-world situations. To address this challenge, we propose Flex-MoE (Flexible Mixture-of-Experts), a new framework designed to flexibly incorporate arbitrary modality combinations while maintaining robustness to missing data. The core idea of Flex-MoE is to first address missing modalities using a new missing modality bank that integrates observed modality combinations with the corresponding missing ones. This is followed by a uniquely designed Sparse MoE framework. Specifically, Flex-MoE first trains experts using samples with all modalities to inject generalized knowledge through the generalized router (G-Router). The S-Router then specializes in handling fewer modality combinations by assigning the top-1 gate to the expert corresponding to the observed modality combination. We evaluate Flex-MoE on the ADNI dataset, which encompasses four modalities in the Alzheimer’s Disease domain, as well as on the MIMIC-IV dataset. The results demonstrate the effectiveness of Flex-MoE, highlighting its ability to model arbitrary modality combinations in diverse missing modality scenarios. Code is available at: https://github.com/UNITES-Lab/flex-moe.}\n}"
    }
  ]
}